# Web Scraping Data and Saving it to CSV

## Overview

Web scraping is the process of automating data collection from the internet, enabling the collection of vast amounts of diverse real-world data. Acquiring data through web scraping can help overcome limitations of traditional datasets, enhancing generalizability and ensuring models remain up-to-date with current trends and information. 

## How we do this:

Some of the time you can just download the html of a page and parse it with BeautifulSoup. 

```
import requests as rq
from bs4 import BeautifulSoup

html = rq.get("https://www.dsmlresearch.org/")
soup = BeautifulSoup(html)
soup.find("h1", {"class": "info"})
```

However, many sites are dynamic, meaning the content is generated by JavaScript. In these cases, we can use Selenium to automate a browser to load the page and then parse the content. Refer to the [web scraping guide](https://github.com/CodesmithLLC/aws-cloud-guides/blob/main/webscraping.md) for more info.

## Assumptions/Considerations

- Put this function into a script cause you'll run it every time you want to update your data.
- You may be rate limited by the website you are scraping. Consider throttling your requests to avoid this.
- Web Scraping is often IO bound, meaning a significant amount of time in executing the script comes from network IO
- When initially requesting a site with Selenium, we must use the XPath to determine what portion of the HTML we want. Look at the difference between full XPath and relative XPath and consider the tradeoffs of using either.

## Variations/Alternatives

There are many different ways we can source data other than Web Scraping:

- APIs 
- Fixed Datasets
- Surveys
- Experiments
- Simulation
